"""Module Overview
-----------------
Rank retrieved passages by *helpfulness* (similarity plus visit frequency),
query an LLM with the top passages, and score its answers. The module also
exposes a dispatcher to run dense, HopRAG and enhanced HopRAG pipelines
end-to-end.


Inputs
------
``query_data`` : List[Dict]
    Question entries with ``question_id``, ``question`` and ``gold_answer``.
``graph`` : :class:`networkx.DiGraph`
    Passage graph whose nodes contain ``text`` and ``query_sim`` attributes.
``passage_metadata``, ``passage_emb``, ``passage_index``
    Retrieval artefacts produced by :mod:`src.b_sparse_dense_representations`.
``model_servers`` : List[str]
    HTTP endpoints for completion models.


Outputs
-------
``results/dense_rag_results.jsonl``
    Predictions generated by :func:`run_dense_rag_baseline`.
``results/dev_results.jsonl`` ######################### why's this called dev? 
    Combined outputs written by :func:`run_pipeline`.


Examples
--------

Example line from ``results/dense_rag_results.jsonl``::

    {
      "query_id": "q1",
      "question": "...",
      "predicted_answer": "...",
      "EM": 1,
      "F1": 1.0,
      "method": "dense_rag"
    }


``results/dev_results.jsonl`` ######################### what does this look like? 




"""

from typing import List, Dict, Optional, Tuple

from typing import List, Dict, Optional, Tuple

import json
import os
import re
import string

import faiss
import networkx as nx
from sentence_transformers import SentenceTransformer

from src.utils import load_jsonl
from src.b_sparse_dense_representations import dataset_rep_paths, load_faiss_index
from src.utils.new_utils import MODEL_SERVERS
from src.a2_text_prep import query_llm
from src.d_traversal import (
    select_seed_passages,
    run_dev_set,
    hoprag_traversal_algorithm,
    enhanced_traversal_algorithm,
)




# ANSWER GENERATION

TOP_K_ANSWER_PASSAGES = 5 

################################################################################################################
# PASSAGE RERANKING AND ANSWER GENERATION 
################################################################################################################



def compute_helpfulness( # helper function for rerank_passages_by_helpfulness()
    vertex_id: str,
    vertex_query_sim: float, # similarity between the passage and the query
    ccount: dict
) -> float:

    """
    Compute a numeric helpfulness score for a passage.

    Args:
        vertex_id: Identifier of the passage vertex.
        vertex_query_sim: Similarity between the passage and the query in ``[0, 1]``.
        ccount: Mapping of passage IDs to visitation counts during traversal.

    Returns:
        float: Helpfulness score in ``[0, 1]`` where higher values indicate
        passages that are both similar to the query and frequently visited.
        The score is the average of ``vertex_query_sim`` and the normalised
        visit count (importance).
    """
    total_visits = sum(ccount.values()) or 1
    importance = ccount.get(vertex_id, 0) / total_visits

    # HopRAG helpfulness formula: (SIM + IMP) / 2
    helpfulness = 0.5 * (vertex_query_sim + importance) # similarity between the passage and the query
    return helpfulness



def rerank_passages_by_helpfulness(
    candidate_passages: List[str],
    query_text: str,
    ccount: dict,
    graph: nx.DiGraph,
    top_k: int = 5
) -> List[Tuple[str, float]]:
    """
    Compute helpfulness scores for candidate passages and return top-k ranked list.
    
    Returns:
        List of tuples: [(passage_id, helpfulness_score), ...]
    """
    reranked = []
    for pid in candidate_passages:
        node = graph.nodes.get(pid, {})
        passage_text = node.get("text", "")
        vertex_query_sim = node.get("query_sim", 0.0)  # make sure this is populated when building graph!

        score = compute_helpfulness(pid, vertex_query_sim, ccount)
        reranked.append((pid, score))

    reranked.sort(key=lambda x: x[1], reverse=True)
    return reranked[:top_k]



def normalise_answer(s: str) -> str:
    """
    
    Lower text and remove punctuation, articles and extra whitespace.
    to prepare answers for EM/F1
    
    
    """
    def remove_articles(text):
        return re.sub(r'\b(a|an|the)\b', ' ', text)

    def white_space_fix(text):
        return ' '.join(text.split())

    def remove_punc(text):
        return ''.join(ch for ch in text if ch not in string.punctuation)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))



def ask_llm_with_passages(
    query_text: str,
    passage_ids: List[str],
    graph: Optional[nx.DiGraph],
    server_url: str,
    max_tokens: int = 100,
    passage_lookup: Optional[Dict[str, str]] = None  # optional for dense mode
) -> Dict[str, str]:
    """Generate an answer from top passages using an LLM server.

    Inputs
    ------
    query_text : str
        User question to be answered.
    passage_ids : List[str]
        Visited passage identifiers ranked by helpfulness.
    graph : Optional[nx.DiGraph]
        Graph containing passage texts. ``None`` if passages are looked up
        elsewhere.
    server_url : str
        URL of the LLM completion endpoint.
    max_tokens : int, optional
        Maximum number of tokens to generate, by default ``100``.
    passage_lookup : Optional[Dict[str, str]]
        Mapping from ``passage_id`` to passage text when ``graph`` is ``None``.

    Outputs
    -------
    Dict[str, str]
        ``{"raw_answer": str, "normalized_answer": str}``.
    """
    passage_texts = []

    for pid in passage_ids:
        if graph:
            passage = graph.nodes[pid].get("text", "")
        elif passage_lookup:
            passage = passage_lookup.get(pid, "")
        else:
            passage = f"[{pid}]"

        passage_texts.append(f"[{pid}]: {passage}")

    prompt = (
        f"Answer the question using the following passages:\n\n"
        + "\n\n".join(passage_texts)
        + f"\n\nQuestion: {query_text}\nAnswer:"
    )

    raw = query_llm(prompt, server_url=server_url, max_tokens=max_tokens)
    norm = normalise_answer(raw)
    return {"raw_answer": raw, "normalised_answer": norm}






def compute_exact_match(
        pred: str, 
        gold: str
        ) -> int:
    """EM is 1 if normalized prediction == normalized gold, else 0."""
    return int(normalise_answer(pred) == normalise_answer(gold))



def compute_f1(
        pred: str, 
        gold: str
        ) -> float:
    """
    Compute token-level F1 between prediction and gold after normalization.
    """
    pred_tokens = normalise_answer(pred).split()
    gold_tokens = normalise_answer(gold).split()

    common = set(pred_tokens) & set(gold_tokens)
    num_same = sum(min(pred_tokens.count(w), gold_tokens.count(w)) for w in common)

    if len(pred_tokens) == 0 or len(gold_tokens) == 0:
        return float(pred_tokens == gold_tokens)

    if num_same == 0:
        return 0.0

    precision = num_same / len(pred_tokens)
    recall = num_same / len(gold_tokens)
    return 2 * precision * recall / (precision + recall)



def evaluate_answers( 
        predictions: dict, 
        gold_answers: dict
        ) -> dict: 
    """
    Evaluate EM and F1 over final answers only.

    predictions: dict of {id: predicted_answer}
    gold_answers: dict of {id: list of gold answers (to allow multiple paraphrases)}

    Returns: dict with overall EM and F1 (%).
    """
    total = len(gold_answers)
    em_total = 0
    f1_total = 0.0

    for qid, gold_list in gold_answers.items():
        pred = predictions.get(qid, "")
        em = max(compute_exact_match(pred, g) for g in gold_list)  
        f1 = max(compute_f1(pred, g) for g in gold_list) 
        em_total += em
        f1_total += f1

    return {
        "EM": 100.0 * em_total / total,
        "F1": 100.0 * f1_total / total
    }








def sweep_thresholds(edges: List[Dict], thresholds: List[float]):
    for t in thresholds:
        filtered = [e for e in edges if e['sim_hybrid'] >= t]
        print(f"Threshold {t:.2f}: {len(filtered)} edges")














def run_dense_rag_baseline(
    query_data: List[Dict],
    passage_metadata: List[Dict],
    passage_emb: np.ndarray,
    passage_index,
    emb_model,
    model_servers: List[str],
    output_path="results/dense_rag_results.jsonl",
    seed_top_k=50,
    alpha=0.5
):
    """
    Dense RAG baseline: 
    - retrieve top-k by cosine/Jaccard hybrid
    - no traversal or graph
    - rerank by cosine + jaccard
    - generate answer
    """
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w") as f:
        pass  # Clear file before writing

    for entry in query_data:
        query_id = entry["question_id"]
        query_text = entry["question"]
        gold_answer = entry["gold_answer"]
        gold_normalised = normalise_answer(gold_answer)

        print(f"\n[Dense RAG] {query_id} - \"{query_text}\"")

        # --- Embed query ---
        query_emb = emb_model.encode(query_text, normalize_embeddings=True)

        # --- Retrieve top-k seed passages ---
        seed_passages = select_seed_passages(
            query_text=query_text,
            query_emb=query_emb,
            passage_metadata=passage_metadata,
            passage_index=passage_index,
            seed_top_k=seed_top_k,
            alpha=alpha
        )

        print(f"[Retrieved] {len(seed_passages)} passages")

        # --- Generate answer from top passages ---
        llm_output = ask_llm_with_passages(
            query_text=query_text,
            passage_ids=seed_passages,
            graph=None,  # don't use graph — will look up text manually below
            server_url=model_servers[0],
            max_tokens=128
        )

        pred_answer = llm_output["normalised_answer"]
        raw_answer = llm_output["raw_answer"]

        # --- Evaluate ---
        em = compute_exact_match(pred_answer, gold_normalised)
        f1 = compute_f1(pred_answer, gold_normalised)

        # --- Save ---
        result = {
            "query_id": query_id,
            "question": query_text,
            "gold_answer": gold_answer,
            "predicted_answer": raw_answer,
            "normalised_pred": pred_answer,
            "normalised_gold": gold_normalised,
            "retrieved_passages": seed_passages,
            "EM": em,
            "F1": round(f1, 4),
            "method": "dense_rag"
        }

        with open(output_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(result, ensure_ascii=False) + "\n")

    print(f"\n[Done] Saved dense RAG results to {output_path}")



def run_pipeline(
    mode: str,
    query_data: List[Dict],
    graph: Optional[nx.DiGraph],
    passage_metadata: List[Dict],
    passage_emb: np.ndarray,
    passage_index,
    emb_model,
    model_servers,
    output_path="results/dev_results.jsonl",
    seed_top_k=50,
    alpha=0.5,
    n_hops=2
):
    """
    Dispatcher to run any of the 3 pipelines:
    - 'dense': Dense-only baseline (no graph)
    - 'hoprag': Standard HopRAG
    - 'enhanced': Enhanced HopRAG
    """
    if mode == "dense":
        run_dense_rag_baseline(
            query_data=query_data,
            passage_metadata=passage_metadata,
            passage_emb=passage_emb,
            passage_index=passage_index,
            emb_model=emb_model,
            model_servers=model_servers,
            output_path=output_path,
            seed_top_k=seed_top_k,
            alpha=alpha
        )
    elif mode == "hoprag":
        run_dev_set(
            query_data=query_data,
            graph=graph,
            passage_metadata=passage_metadata,
            passage_emb=passage_emb,
            passage_index=passage_index,
            emb_model=emb_model,
            model_servers=model_servers,
            output_path=output_path,
            seed_top_k=seed_top_k,
            alpha=alpha,
            n_hops=n_hops,
            traveral_alg=hoprag_traversal_algorithm
        )
    elif mode == "enhanced":
        run_dev_set(
            query_data=query_data,
            graph=graph,
            passage_metadata=passage_metadata,
            passage_emb=passage_emb,
            passage_index=passage_index,
            emb_model=emb_model,
            model_servers=model_servers,
            output_path=output_path,
            seed_top_k=seed_top_k,
            alpha=alpha,
            n_hops=n_hops,
            traveral_alg=enhanced_traversal_algorithm
        )
    else:
        raise ValueError(f"Unknown mode: {mode}")









if __name__ == "__main__":
    # --- Required objects: define or load them before this block ---
    # These must be defined earlier in your script or imported:
    # query_data: List[Dict]
    # graph: nx.DiGraph
    # passage_metadata: List[Dict]
    # passage_emb: np.ndarray
    # passage_index: FAISS or similar
    # emb_model: embedding model with `.encode()`
    # model_servers: List[str]

    seed_top_k = 50
    alpha = 0.5
    n_hops = 2
    output_base = "results"

    os.makedirs(output_base, exist_ok=True)

    print("\n========== Running DENSE RAG ==========")
    run_pipeline(
        mode="dense",
        query_data=query_data,
        graph=None,
        passage_metadata=passage_metadata,
        passage_emb=passage_emb,
        passage_index=passage_index,
        emb_model=emb_model,
        model_servers=model_servers,
        output_path=os.path.join(output_base, "dev_dense.jsonl"),
        seed_top_k=seed_top_k,
        alpha=alpha
    )

    print("\n========== Running STANDARD HopRAG ==========")
    run_pipeline(
        mode="hoprag",
        query_data=query_data,
        graph=graph,
        passage_metadata=passage_metadata,
        passage_emb=passage_emb,
        passage_index=passage_index,
        emb_model=emb_model,
        model_servers=model_servers,
        output_path=os.path.join(output_base, "dev_hoprag.jsonl"),
        seed_top_k=seed_top_k,
        alpha=alpha,
        n_hops=n_hops
    )

    print("\n========== Running ENHANCED HopRAG ==========")
    run_pipeline(
        mode="enhanced",
        query_data=query_data,
        graph=graph,
        passage_metadata=passage_metadata,
        passage_emb=passage_emb,
        passage_index=passage_index,
        emb_model=emb_model,
        model_servers=model_servers,
        output_path=os.path.join(output_base, "dev_enhanced.jsonl"),
        seed_top_k=seed_top_k,
        alpha=alpha,
        n_hops=n_hops
    )

    print("\n✅ All modes complete.")
